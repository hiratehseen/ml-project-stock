{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) using BiLSTM-LSTM Model\n",
    "In this code, we will explore the task of Named Entity Recognition (NER) and build a BiLSTM-LSTM model for NER using the TensorFlow library. NER is a natural language processing (NLP) task that involves identifying and classifying named entities, such as person names, locations, organizations, and other types of named entities, in text data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Named Entity Recognition (NER)?\n",
    "Named Entity Recognition (NER) is a subtask of information extraction that aims to locate and classify named entities in text into predefined categories. The named entities can be proper nouns, such as names of people, organizations, locations, or expressions of time, quantities, monetary values, percentages, etc. NER is widely used in various NLP applications, including question answering, information retrieval, machine translation, and sentiment analysis.\n",
    "\n",
    "### Dataset\n",
    "For this task, we will use a dataset called ner_dataset.csv. This dataset contains sentences with corresponding words and their named entity tags. The data has the following columns: Sentence #, Word, POS (Part of Speech), and Tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model, pad_sequences, to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from itertools import chain\n",
    "from numpy.random import seed\n",
    "\n",
    "# set seeds for reproducibility\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('ner_dataset.csv', encoding= 'unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Before we can train our NER model, we need to preprocess and prepare the data. This involves converting words and tags into numerical representations, padding sequences, and splitting the data into training, validation, and testing sets.\n",
    "\n",
    "To prepare the data, we'll define a few helper functions. Let's go through them step by step:\n",
    "\n",
    "### 1. Get Unique Words and Tags\n",
    "We'll start by defining a function that extracts unique words and tags from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get unique words and tags from data\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the data and the type of token (word or tag) as inputs and returns dictionaries to map tokens to indices and vice versa.\n",
    "\n",
    "### 2. Map Words and Tags to Indices\n",
    "Next, we'll map words and tags to their corresponding indices using the dictionaries obtained from the previous step. We add two new columns to the data DataFrame: Word_idx and Tag_idx, which contain the indices for words and tags, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')\n",
    "data['Word_idx'] = data['Word'].map(token2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx)\n",
    "data_fillna = data.fillna(method='ffill', axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Group Data by Sentences\n",
    "Since our model will process sentences, we need to group the data by sentences. We'll use the groupby function from pandas to group the data based on the \"Sentence #\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hira\\AppData\\Local\\Temp\\ipykernel_1044\\3689941710.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  data_group = data_fillna.groupby(['Sentence #'],as_index=False)['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n"
     ]
    }
   ],
   "source": [
    "# Groupby and collect columns\n",
    "data_group = data_fillna.groupby(['Sentence #'],as_index=False)['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grouped data is stored in the data_group DataFrame, which contains the sentences, words, parts of speech (POS), tags, word indices, and tag indices.\n",
    "\n",
    "### 4. Pad and Split the Data\n",
    "Next, we'll define a function that pads the token sequences and splits the data into training, validation, and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens length: 32372 \n",
      "train_tokens length: 32372 \n",
      "test_tokens length: 4796 \n",
      "test_tags: 4796 \n",
      "val_tokens: 10791 \n",
      "val_tags: 10791\n"
     ]
    }
   ],
   "source": [
    "def get_pad_train_test_val(data_group, data):\n",
    "\n",
    "    #get max token and tag length\n",
    "    n_token = len(list(set(data['Word'].to_list())))\n",
    "    n_tag = len(list(set(data['Tag'].to_list())))\n",
    "\n",
    "    #Pad tokens (X var)    \n",
    "    tokens = data_group['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "\n",
    "    #Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = data_group['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    #Split train, test and validation set\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntrain_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "        '\\nval_tokens:', len(val_tokens),\n",
    "        '\\nval_tags:', len(val_tags),\n",
    "    )\n",
    "    \n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n",
    "\n",
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the grouped data and the original data as inputs and returns the padded tokens and tags along with the split train, validation, and test sets.\n",
    "\n",
    "### 5. Prepare Model Input Parameters\n",
    "Finally, we'll define some input parameters for our NER model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(list(set(data['Word'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, input_dim represents the vocabulary size (number of unique words plus one for padding), output_dim represents the dimensionality of the word embeddings, input_length represents the maximum sequence length, and n_tags represents the number of unique tags.\n",
    "\n",
    "Now that we have prepared the data, let's move on to building the BiLSTM-LSTM model.\n",
    "\n",
    "## Build the BiLSTM-LSTM Model\n",
    "Our NER model will consist of an embedding layer, a bidirectional LSTM layer, a unidirectional LSTM layer, and a time-distributed dense layer. We'll use the Keras Sequential API to build the model.\n",
    "\n",
    "Let's define a function to create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"sigmoid\")))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we first add an embedding layer that converts the input word indices into dense word embeddings. Then, we add a bidirectional LSTM layer to capture the contextual information from both directions. We follow it with a unidirectional LSTM layer for further sequence modeling. Finally, we add a time-distributed dense layer that predicts the tags for each word in the input sequence. The model is compiled with the categorical cross-entropy loss function and the Adam optimizer.\n",
    "### Train the Model\n",
    "With the model architecture defined, we can now train the model on our prepared data. We'll define a function that trains the model for a specified number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 104, 64)           2251456   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 104, 128)         66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 104, 64)           49408     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 104, 17)          1105      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,368,017\n",
      "Trainable params: 2,368,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "26/26 [==============================] - 263s 10s/step - loss: 2.0930 - accuracy: 0.9273 - val_loss: 1.2324 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 289s 11s/step - loss: 0.7800 - accuracy: 0.9677 - val_loss: 0.4472 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 271s 10s/step - loss: 0.3474 - accuracy: 0.9677 - val_loss: 0.2776 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 300s 12s/step - loss: 0.2574 - accuracy: 0.9677 - val_loss: 0.2377 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 307s 12s/step - loss: 0.2302 - accuracy: 0.9677 - val_loss: 0.2169 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 320s 12s/step - loss: 0.2123 - accuracy: 0.9677 - val_loss: 0.2027 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 321s 12s/step - loss: 0.2007 - accuracy: 0.9677 - val_loss: 0.1938 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 332s 13s/step - loss: 0.1929 - accuracy: 0.9677 - val_loss: 0.1874 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 325s 13s/step - loss: 0.1870 - accuracy: 0.9677 - val_loss: 0.1826 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 344s 13s/step - loss: 0.1822 - accuracy: 0.9677 - val_loss: 0.1784 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 322s 12s/step - loss: 0.1775 - accuracy: 0.9677 - val_loss: 0.1735 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 370s 14s/step - loss: 0.1721 - accuracy: 0.9677 - val_loss: 0.1678 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 353s 14s/step - loss: 0.1665 - accuracy: 0.9677 - val_loss: 0.1620 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 377s 15s/step - loss: 0.1597 - accuracy: 0.9677 - val_loss: 0.1534 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 358s 14s/step - loss: 0.1505 - accuracy: 0.9677 - val_loss: 0.1418 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 381s 15s/step - loss: 0.1382 - accuracy: 0.9677 - val_loss: 0.1273 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 376s 14s/step - loss: 0.1242 - accuracy: 0.9677 - val_loss: 0.1147 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 382s 15s/step - loss: 0.1123 - accuracy: 0.9677 - val_loss: 0.1054 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 352s 14s/step - loss: 0.1035 - accuracy: 0.9677 - val_loss: 0.0993 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 384s 15s/step - loss: 0.0978 - accuracy: 0.9677 - val_loss: 0.0953 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 383s 15s/step - loss: 0.0939 - accuracy: 0.9677 - val_loss: 0.0926 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 389s 15s/step - loss: 0.0910 - accuracy: 0.9677 - val_loss: 0.0905 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 404s 16s/step - loss: 0.0887 - accuracy: 0.9678 - val_loss: 0.0888 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 411s 16s/step - loss: 0.0867 - accuracy: 0.9679 - val_loss: 0.0872 - val_accuracy: 0.9685\n",
      "26/26 [==============================] - 404s 16s/step - loss: 0.0849 - accuracy: 0.9681 - val_loss: 0.0857 - val_accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(25):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss\n",
    "\n",
    "results = pd.DataFrame()\n",
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "plot_model(model_bilstm_lstm)\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we iterate over a specified number of epochs and train the model using the training tokens and tags. We store the loss values in a list for further analysis.\n",
    "\n",
    "### Testing the Model\n",
    "To test our NER model, let's define a sample text and visualize the named entities using the spacy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is the capital and largest city of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    England\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United Kingdom\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", with a population of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    just under 9 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". </br> It stands on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the River Thames\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    south-east\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    England\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " at the head of a \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    50-mile\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    80 km\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       ") estuary down to \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the North Sea\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", and has been a major settlement for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two millennia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". </br> \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The City of London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", its ancient core and financial centre, was founded by the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Romans\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " as \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Londinium\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " and retains its medieval boundaries. </br> \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The City of Westminster\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", to the west of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the City of London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", has for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    centuries\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " hosted the national government and parliament. </br> Since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the 19th century\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", the name &quot;\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "&quot; has also referred to the metropolis around this core, historically split between the counties of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Middlesex\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", Essex, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Surrey\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kent\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hertfordshire\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", which since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1965\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " has largely comprised \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Greater London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", which is governed by \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    33\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " local authorities and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Greater London Authority\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = nlp('London is the capital and largest city of England and the United Kingdom, with a population of just under 9 million. \\n It stands on the River Thames in south-east England at the head of a 50-mile (80 km) estuary down to the North Sea, and has been a major settlement for two millennia. \\n The City of London, its ancient core and financial centre, was founded by the Romans as Londinium and retains its medieval boundaries. \\n The City of Westminster, to the west of the City of London, has for centuries hosted the national government and parliament. \\n Since the 19th century, the name \"London\" has also referred to the metropolis around this core, historically split between the counties of Middlesex, Essex, Surrey, Kent, and Hertfordshire, which since 1965 has largely comprised Greater London, which is governed by 33 local authorities and the Greater London Authority.')\n",
    "displacy.render(text, style = 'ent', jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the en_core_web_sm model from spacy to tokenize the text and extract named entities. The displacy.render function is used to visualize the named entities in the text.\n",
    "\n",
    "# Conclusion\n",
    "We explored the task of Named Entity Recognition (NER) and built a BiLSTM-LSTM model for NER using the TensorFlow library. We covered the data preparation steps, model architecture, training, and testing. NER is a powerful technique in NLP and can be applied to various real-world applications for extracting structured information from unstructured text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
